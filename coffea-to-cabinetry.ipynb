{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coffea to cabinetry\n",
    "Template histograms for statistical analysis are defined implicitly in `cabinetry`.\n",
    "Below is a partial example:\n",
    "```yaml\n",
    "Regions:\n",
    "  - Name: \"Signal_region\"\n",
    "    Variable: \"lep_pt\"\n",
    "    Filter: \"HT_jets >= 1000\"\n",
    "    Binning: [200, 300, 400, 500, 600]\n",
    "        \n",
    "  - Name: \"Control_region\"\n",
    "    Variable: \"lep_pt\"\n",
    "    Filter: \"HT_jets < 1000\"\n",
    "    Binning: [0, 300, 600]\n",
    "\n",
    "Samples:\n",
    "  - Name: \"Signal\"\n",
    "    Tree: \"signal\"\n",
    "    SamplePaths: \"prediction.root\"\n",
    "    Weight: \"weight\"\n",
    "\n",
    "  - Name: \"Background\"\n",
    "    Tree: \"background\"\n",
    "    SamplePaths: \"prediction.root\"\n",
    "    Weight: \"weight\"\n",
    "\n",
    "Systematics:\n",
    "  - Name: \"WeightBasedModeling\"\n",
    "    Up:\n",
    "      Weight: \"weight_up\"\n",
    "    Down:\n",
    "      Weight: \"0.7*weight_down\"\n",
    "    Samples: \"Background\"\n",
    "    Type: \"NormPlusShape\"\n",
    "```\n",
    "There are two regions/channels: `Signal_region` and `Control_region`.\n",
    "For both channels, nominal histograms of the `Signal` and `Background` samples are needed (4 templates).\n",
    "Additionally, for the `Background` sample a systematic variation `WeightBasedModeling` is needed (4 more templates, 1 up/down in each region).\n",
    "In total, the above implicitly contains instructions for building 8 different template histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current `cabinetry` approach\n",
    "At the moment, `cabinetry` handles the template building like this:\n",
    "- loop over all regions\n",
    "  - loop over all samples\n",
    "    - determine which systematic variations `[s1, s2, s3, ...]` affect given sample (in given region)\n",
    "    - loop over nominal template `n` and systematics, send instructions for one template at a time (`[n, s1, ...]`) to a backend\n",
    "    - receive single template from backend, save to file\n",
    "    \n",
    "The backend is currently `uproot`-based ([link](https://github.com/alexander-held/cabinetry/blob/c8668005e899556675b5e646e127908849bfe597/src/cabinetry/contrib/histogram_creation.py#L10-L83)).\n",
    "The relevant function turns histogram-building instructions\n",
    "```python\n",
    "def from_uproot(\n",
    "    ntuple_paths: List[pathlib.Path],\n",
    "    pos_in_file: str,\n",
    "    variable: str,\n",
    "    bins: np.ndarray,\n",
    "    weight: Optional[str] = None,\n",
    "    selection_filter: Optional[str] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "```\n",
    "into a histogram (yields + stat. uncertainties).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive coffea port\n",
    "The same method can be used with `coffea`.\n",
    "`cabinetry` generates histogram building instructions and sends them off to coffea, one instruction at a time.\n",
    "A generic processor is used to handle those instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import awkward as ak\n",
    "from coffea import hist, processor\n",
    "from coffea.nanoevents import BaseSchema\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, channel_info, template_info):\n",
    "        if channel_info is None or template_info is None:\n",
    "            raise ValueError\n",
    "\n",
    "        # set up accumulator from histogram info\n",
    "        # channels are in dict and not an axis, since binning per channel can vary\n",
    "        self._accumulator = processor.dict_accumulator()\n",
    "        for ch in channel_info:\n",
    "            self._accumulator.update(\n",
    "                {\n",
    "                    ch[\"name\"]: hist.Hist(\n",
    "                        \"events\",\n",
    "                        hist.Cat(\"dataset\", \"dataset\"),\n",
    "                        hist.Cat(\"template\", \"systematic template\"),\n",
    "                        hist.Bin(ch[\"variable\"], ch[\"observable_label\"], ch[\"binning\"]),\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        self.channel_info = channel_info\n",
    "        self.template_info = template_info\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        out = self.accumulator.identity()\n",
    "\n",
    "        # dataset from metadata\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "\n",
    "        # loop over channels\n",
    "        for ch in self.channel_info:\n",
    "            # get relevant info for building histogram from metadata\n",
    "            channel_name = ch[\"name\"]\n",
    "            variable = ch[\"variable\"]\n",
    "\n",
    "            # loop over templates\n",
    "            for template in self.template_info[dataset]:\n",
    "                template_name = template[\"name\"]\n",
    "                selection_filter = template[\"selection_filter\"]\n",
    "\n",
    "                # apply cuts\n",
    "                events_cut = events[eval(selection_filter, {}, events)]\n",
    "\n",
    "                observables = eval(variable, {}, events_cut)\n",
    "\n",
    "                if template[\"weight\"] is not None:\n",
    "                    weight_expression = template[\"weight\"]\n",
    "                    weights = eval(weight_expression, {}, events_cut)\n",
    "                else:\n",
    "                    weights = np.ones(len(observables))\n",
    "\n",
    "                out[channel_name].fill(\n",
    "                    dataset=dataset,\n",
    "                    template=template_name,\n",
    "                    weight=weights,\n",
    "                    **{variable: observables},\n",
    "                )\n",
    "        return out\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This processor is steered by instructions `channel_info, template_info` provided in its constructor.\n",
    "It is able to process multiple channels and templates at once, but not in any way optimized for this and `cabinetry` currently does not make use of this feature.\n",
    "\n",
    "The processor can be called similarly to the `uproot` backend.\n",
    "The same information is provided, and converted into dictionaries to hand to the processor constructor.\n",
    "Note that the `build_single_histogram` signature matches `from_uproot` shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_histogram(\n",
    "    ntuple_paths: List[pathlib.Path],\n",
    "    pos_in_file: str,\n",
    "    variable: str,\n",
    "    bins: np.ndarray,\n",
    "    weight: Optional[str] = None,\n",
    "    selection_filter: Optional[str] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # sample can have generic name, not needed here\n",
    "    # need to convert list of paths to list of strings\n",
    "    samples = {\n",
    "        \"generic_name\": {\n",
    "            \"treename\": pos_in_file,\n",
    "            \"files\": [str(p) for p in ntuple_paths],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # template: one at a time, template name not needed\n",
    "    template_info = {\n",
    "        \"generic_name\": [\n",
    "            {\n",
    "                \"name\": \"generic_template_name\",\n",
    "                \"weight\": weight,\n",
    "                \"selection_filter\": selection_filter,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # channel info: more generic info that is not needed here\n",
    "    channel_info = [\n",
    "        {\n",
    "            \"name\": \"generic_channel_name\",\n",
    "            \"variable\": variable,\n",
    "            \"observable_label\": \"generic_label\",  # for cosmetics\n",
    "            \"binning\": bins,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    result = processor.run_uproot_job(\n",
    "        samples,\n",
    "        None,  # tree name is specified in fileset\n",
    "        MyProcessor(channel_info, template_info),\n",
    "        processor.iterative_executor,\n",
    "        {\"schema\": BaseSchema},\n",
    "    )\n",
    "\n",
    "    yields, variance = result[\"generic_channel_name\"].values(sumw2=True)[\n",
    "        (\"generic_name\", \"generic_template_name\")\n",
    "    ]\n",
    "    return yields, np.sqrt(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of information in the dictionaries provided to the `coffea` processor is filled with generic values, since it is not actually relevant for this specific task of building histograms for building a statistical model.\n",
    "I expect that the design of this interface will evolve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing for performance\n",
    "Right now `cabinetry` creates instructions for building a single histogram, sends those to a backend, waits for the creation of the histogram, saves it, and then continues with the next histogram.\n",
    "To parallelize this, `cabinetry` should instead create a list of all instructions and then send them of to a backend, allowing to process them asynchronously.\n",
    "After all instructions are processed, the results need to be gathered and saved.\n",
    "\n",
    "Once we have a list of histogram building instructions, we could imagine a new layer between `cabinetry` and `coffea`.\n",
    "This layer could analyze the instructions and group them together to allow for efficient processing, considering CPU/memory/disk.\n",
    "\n",
    "**How should we approach such a layer? Does this seem like the right way forward in general?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open questions\n",
    "Traditionally, users of `coffea` seem to implement much more in their processors than just simple selections.\n",
    "They may include complex event reconstructions methods.\n",
    "Those are not things that fit well into the `cabinetry` configuration, so one may imagine that people interested in using custom processors inherit from some modified processor base class that handles the basic features needed (processing templates from metadata handed to the processor constructor), but also allows them to implement more advanced functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example\n",
    "Below a simple example of using `coffea` to build some histograms is shown.\n",
    "The relevant input data is generated with [a utility included in `cabinetry`](https://github.com/alexander-held/cabinetry/blob/c8668005e899556675b5e646e127908849bfe597/util/create_ntuples.py).\n",
    "This example processes multiple template histograms in a single processor (nominal `background` histogram as well as a systeamatic variation together).\n",
    "\n",
    "First, define all information needed by the processor:\n",
    "- which samples to process\n",
    "- which template histograms to build per sample\n",
    "- which channels to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"signal\": {\n",
    "        \"treename\": \"signal\",\n",
    "        \"files\": [\"../cabinetry/ntuples/prediction.root\"],\n",
    "    },\n",
    "    \"background\": {\n",
    "        \"treename\": \"background\",\n",
    "        \"files\": [\"../cabinetry/ntuples/prediction.root\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# list of templates per sample, each template specified via dict (name/selection/weight)\n",
    "# TODO: allow channel-specific systematics\n",
    "# TODO: could this be integrated with the sample list above (with files etc.)?\n",
    "#       - https://github.com/CoffeaTeam/coffea/issues/479\n",
    "# TODO: how to handle systematics that use different files\n",
    "template_info = {\n",
    "    \"signal\": [\n",
    "        {\n",
    "            \"name\": \"nominal\",\n",
    "            \"weight\": \"weight\",\n",
    "            \"selection_filter\": \"lep_charge == 1\",\n",
    "        }\n",
    "    ],\n",
    "    \"background\": [\n",
    "        {\n",
    "            \"name\": \"nominal\",\n",
    "            \"weight\": \"weight\",\n",
    "            \"selection_filter\": \"lep_charge == 1\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"WeightBasedModeling\",\n",
    "            \"weight\": \"weight_up\",\n",
    "            \"selection_filter\": \"lep_charge == 1\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# histogram-related info contains channel name, not needed for processing but possibly\n",
    "# useful for bookkeeping\n",
    "# list of channels, dicts per channel contain relevant info\n",
    "channel_info = [\n",
    "    {\n",
    "        \"name\": \"Signal_region\",\n",
    "        \"variable\": \"jet_pt\",\n",
    "        \"observable_label\": \"jet $p_T$ [GeV]\",  # for cosmetics\n",
    "        \"binning\": [200, 300, 400, 500, 600],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run `coffea`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d869eefefb38404b9a843a6745d9bbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/2 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404bbf2d731e453c95985d49e4a9b95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = processor.run_uproot_job(\n",
    "    samples,\n",
    "    None,  # tree name is specified in fileset\n",
    "    MyProcessor(channel_info, template_info),\n",
    "    processor.iterative_executor,\n",
    "    {\"schema\": BaseSchema},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally read the histogram content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel Signal_region\n",
      "  dataset: signal\n",
      "    template: nominal\n",
      "      yields: [ 0.          1.58536913 23.6164268  24.54892223]\n",
      "      stdev: [0.         0.19931166 0.77410713 0.78830503]\n",
      "  dataset: background\n",
      "    template: nominal\n",
      "      yields: [112.73896936 128.62169539  88.10700838  55.24607072]\n",
      "      stdev: [4.76136678 5.10645036 4.21104367 3.34933335]\n",
      "    template: WeightBasedModeling\n",
      "      yields: [ 98.13569365 154.1222757  135.20449815 103.14744392]\n",
      "      stdev: [4.17025569 6.14088444 6.47920695 6.2581315 ]\n"
     ]
    }
   ],
   "source": [
    "for ch in channel_info:\n",
    "    print(f\"channel {ch['name']}\")\n",
    "    for ds in template_info.keys():\n",
    "        print(f\"  dataset: {ds}\")\n",
    "        for tem in template_info[ds]:\n",
    "            template_name = tem[\"name\"]\n",
    "            print(f\"    template: {template_name}\")\n",
    "            yields, variance = result[ch[\"name\"]].values(sumw2=True)[\n",
    "                (ds, template_name)\n",
    "            ]\n",
    "            print(f\"      yields: {yields}\")\n",
    "            print(f\"      stdev: {np.sqrt(variance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyhf)",
   "language": "python",
   "name": "pyhf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
